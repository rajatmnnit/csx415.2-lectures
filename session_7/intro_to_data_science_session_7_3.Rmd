---
title: "Introduction to Data Science"
subtitle: "Session 7.3"
author: "Robert Clements"
date: ""
output:
  xaringan::moon_reader:
    css: rc_css.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align = "center", fig.asp=.5, message = FALSE, warning = FALSE)
```

```{r, warning=FALSE,message=FALSE, echo=FALSE}
library(tidyverse)
```

### Session 7.3 Outline

- Practice machine learning with Kaggle
- Evaluating predictive models (regression)
- The bias-variance tradeoff
  + *Modern Data Science with R - Ch 8.4*
  + *Introduction to Statistical Learning - Ch 2.2 and Ch 5.1*
---
class: inverse, center, middle
# Kaggle
---
### Kaggle

[https://www.kaggle.com/](https://www.kaggle.com/) - a place for machine learning competitions and learning.
---
### Kaggle criticisms

Many people like to criticize Kaggle as not being good preparation for data scientists. 

Kaggle's focus is on **machine learning**, and squeezing out every last drop of predictive power out of pre-cleaned, collected datasets with a clearly defined problem. 

Recall that most data scientists spend very little time on doing machine learning - and that machine learning is increasingly getting easier and more automated.

It is typically not productive to spend weeks tuning a model to get that little bit of extra predictive power out of it.

All of that said, we can still use Kaggle to help us learn about machine learning.
---
class: inverse, center, middle
# Evaluating predictive models (regression)
---
### Modeling

There are many different ways of saying *fitting a model*:

- Statisticians will usually say **model fitting**

- Geoscientists will usually say **model inversion**

- Machine learning folks will usually say **model training**

- Data scientists will say either **model fitting** or **model training**, but never **model inversion**...that's just weird.
---
### Model data sets

We sometimes split our data into different sets for different purposes:

- **training set** - the set of data used for training/fitting a model

- **validation set** - the set of data used to help tune the model (more on this later)

- **test set** - the set of data used for estimating the *predictive performance* of your model on brand new future data
---
### Why all of the different sets of data?

To avoid overfitting!!

Every model is *fit* to a set of training data - the model *learns* the optimal set of parameters based on this data.

If your model fits your **training** data well, but then performs terribly on your **testing** data, your model has *overfit* your training data.
---
### Regression model evaluation metrics

$y$ = true value of $Y$  
$\hat{y}$ = predicted value of $Y$

$R^2$ - the coefficient of determination

RMSE - root mean squared error

$$RMSE = \sqrt{\frac{1}{n}\sum_{i = 1}^{n}(y_{i} - \hat{y}_{i})^2}$$

MAE - mean absolute error

$$MAE = \frac{1}{n}\sum_{i = 1}^{n}|y_{i} - \hat{y}_{i}|$$

---
class: inverse, middle, center
# The bias-variance tradeoff
---
### The bias-variance tradeoff

What is *bias*?

*bias* is the amount of error introduced by trying to estimate a complex relationship using a simpler model

```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(tidyverse)
x <- runif(400, -5, 5)
y <- 10 + 3*sin(x) - 1*x + rnorm(400, mean = 0, sd = 3)
plot_data <- tibble(x,y)
```

.pull-left[
```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.height=200}
plot_data %>% ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  geom_smooth(method = 'lm', se = FALSE, size = 3) +
  ggtitle('high bias') +
  theme_bw()
```
]
.pull-right[
```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_data %>% ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE, size = 3) +
  ggtitle('low bias') +
  theme_bw()
```
]
---
### The bias-variance tradeoff

What is *variance*?

*variance* is the amount that our model, $\large \hat{f}(X)$ would change if we used a different training set.

.pull-left[
```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.height=200}
plot_data %>% ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  geom_smooth(span = .05, se = FALSE, size = 3) +
  ggtitle('high variance') +
  theme_bw()
```
]
.pull-right[
```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_data %>% ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE, size = 3) +
  ggtitle('low variance') +
  theme_bw()
```
]
---
### The bias-variance tradeoff

What is *variance*?

*variance* is the amount that our model, $\large \hat{f}(X)$ would change if we used a different training set.

.pull-left[
```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.height=200}

x2 <- runif(400, -5, 5)
y2 <- 10 + 3*sin(x2) - 1*x2 + rnorm(400, mean = 0, sd = 3)
plot_data_2 <- tibble(x2,y2)

plot_data_2 %>% ggplot(aes(x = x2, y = y2)) + 
  geom_point(alpha = .5) +
  geom_smooth(span = .05, se = FALSE, size = 3) +
  ggtitle('high variance') +
  theme_bw()
```
]
.pull-right[
```{r, echo = FALSE, warning=FALSE, message=FALSE}
x3 <- runif(400, -5, 5)
y3 <- 10 + 3*sin(x3) - 1*x3 + rnorm(400, mean = 0, sd = 3)
plot_data_3 <- tibble(x3,y3)

plot_data_3 %>% ggplot(aes(x = x3, y = y3)) + 
  geom_point(alpha = .5) +
  geom_smooth(span = .05, se = FALSE, size = 3) +
  ggtitle('high variance') +
  theme_bw()
```
]
---
### The bias-variance tradeoff

Let's illustrate the tradeoff with this simple example:

```{r, echo = FALSE}
# x <- runif(400, -5, 5)
# y <- 10 + 2*sin(x) - .5*x + rnorm(400)
set <- c(rep('train', 300), rep('test', 100))
plot_data <- tibble(x,y, set)
```

```{r, echo = FALSE}
plot_data %>% ggplot(aes(x = x, y = y, color = set)) +
  geom_point() +
  theme_bw()
```
---
### The bias-variance tradeoff

.pull-left[
```{r, echo = FALSE}
plot_data %>% filter(set == 'train') %>% ggplot(aes(x = x, y = y)) +
  geom_point(color = 'blue') +
  ggtitle('TRAINING') +
  theme_bw()
```
]
.pull-right[
```{r, echo = FALSE}
plot_data %>% filter(set == 'test') %>% ggplot(aes(x = x, y = y)) +
  geom_point(color = 'red') +
  ggtitle('TESTING') +
  theme_bw()
```
]
---
### The bias-variance tradeoff

```{r, echo = FALSE}
yint <- plot_data %>% filter(set == 'train') %>% .[['y']] %>% mean()
rmse <- plot_data %>% filter(set == 'test') %>% summarize(rmse = sqrt(mean((y - yint)^2)))
```

High bias, low variance

Testing set RMSE = `r rmse$rmse`

.pull-left[
```{r, echo = FALSE}
yint <- plot_data %>% filter(set == 'train') %>% .[['y']] %>% mean()
plot_data %>% filter(set == 'train') %>% ggplot(aes(x = x, y = y)) +
  geom_point(color = 'blue') +
  geom_hline(yintercept = yint, size = 3) +
  ggtitle('TRAINING') +
  theme_bw()
```
]
.pull-right[
```{r, echo = FALSE}
plot_data %>% filter(set == 'test') %>% ggplot(aes(x = x, y = y)) +
  geom_point(color = 'red') +
  geom_hline(yintercept = yint, size = 3) +
  ggtitle('TESTING') +
  theme_bw()
```
]
---
### The bias-variance tradeoff

```{r, echo = FALSE}
loe <- loess(formula = y ~ x, data = plot_data %>% filter(set == 'train'))
preds <- predict(loe, newdata = plot_data %>% filter(set == 'test'))
rmse <- plot_data %>% filter(set == 'test') %>% summarize(rmse = sqrt(mean((y - preds)^2, na.rm = TRUE)))
```

low bias, low variance

Testing set RMSE = `r rmse$rmse`

.pull-left[
```{r, echo = FALSE}
plot_data %>% filter(set == 'train') %>% ggplot(aes(x = x, y = y)) +
  geom_point(color = 'blue') +
  geom_smooth(se = FALSE, size = 3) +
  ggtitle('TRAINING') +
  theme_bw()
```
]
.pull-right[
```{r, echo = FALSE}
plot_data %>% filter(set == 'train') %>% ggplot(aes(x = x, y = y)) +
  geom_smooth(se = FALSE, size = 3) +
  geom_point(data = plot_data %>% filter(set == 'test'), color = 'red') +
  ggtitle('TESTING') +
  theme_bw()
```
]
---
### The bias-variance tradeoff

```{r, echo = FALSE, warning = FALSE, message = FALSE}
loe <- loess(formula = y ~ x, data = plot_data %>% filter(set == 'train'), span = .03)
preds <- predict(loe, newdata = plot_data %>% filter(set == 'test'))
rmse <- plot_data %>% filter(set == 'test') %>% summarize(rmse = sqrt(mean((y - preds)^2, na.rm = TRUE)))
```

low bias, high variance

Testing set RMSE = `r rmse$rmse`

.pull-left[
```{r, echo = FALSE, warning = FALSE, message = FALSE}
plot_data %>% filter(set == 'train') %>% ggplot(aes(x = x, y = y)) +
  geom_point(color = 'blue') +
  geom_smooth(span = .03, se = FALSE, size = 3) +
  ggtitle('TRAINING') +
  theme_bw()
```
]
.pull-right[
```{r, echo = FALSE}
plot_data %>% filter(set == 'train') %>% ggplot(aes(x = x, y = y)) +
  geom_smooth(span = .03, se = FALSE, size = 3) +
  geom_point(data = plot_data %>% filter(set == 'test'), color = 'red') +
  ggtitle('TESTING') +
  theme_bw()
```
]
---
### The bias-variance tradeoff

We want a model with:

low bias and low variance

In practice, why would this be hard to find?

---
class: inverse, center, middle

# End of Session 7.3
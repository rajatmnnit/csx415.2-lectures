<!DOCTYPE html>
<html>
  <head>
    <title>Introduction to Data Science</title>
    <meta charset="utf-8">
    <meta name="author" content="Robert Clements" />
    <link rel="stylesheet" href="rc_css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Introduction to Data Science
## Session 7.3
### Robert Clements

---






### Session 7.3 Outline

- Practice machine learning with Kaggle
- Evaluating predictive models (regression)
- The bias-variance tradeoff
  + *Modern Data Science with R - Ch 8.4*
  + *Introduction to Statistical Learning - Ch 2.2 and Ch 5.1*
---
class: inverse, center, middle
# Kaggle
---
### Kaggle

[https://www.kaggle.com/](https://www.kaggle.com/) - a place for machine learning competitions and learning.
---
### Kaggle criticisms

Many people like to criticize Kaggle as not being good preparation for data scientists. 

Kaggle's focus is on **machine learning**, and squeezing out every last drop of predictive power out of pre-cleaned, collected datasets with a clearly defined problem. 

Recall that most data scientists spend very little time on doing machine learning - and that machine learning is increasingly getting easier and more automated.

It is typically not productive to spend weeks tuning a model to get that little bit of extra predictive power out of it.

All of that said, we can still use Kaggle to help us learn about machine learning.
---
class: inverse, center, middle
# Evaluating predictive models (regression)
---
### Modeling

There are many different ways of saying *fitting a model*:

- Statisticians will usually say **model fitting**

- Geoscientists will usually say **model inversion**

- Machine learning folks will usually say **model training**

- Data scientists will say either **model fitting** or **model training**, but never **model inversion**...that's just weird.
---
### Model data sets

We sometimes split our data into different sets for different purposes:

- **training set** - the set of data used for training/fitting a model

- **validation set** - the set of data used to help tune the model (more on this later)

- **test set** - the set of data used for estimating the *predictive performance* of your model on brand new future data
---
### Why all of the different sets of data?

To avoid overfitting!!

Every model is *fit* to a set of training data - the model *learns* the optimal set of parameters based on this data.

If your model fits your **training** data well, but then performs terribly on your **testing** data, your model has *overfit* your training data.
---
### Regression model evaluation metrics

`\(y\)` = true value of `\(Y\)`  
`\(\hat{y}\)` = predicted value of `\(Y\)`

`\(R^2\)` - the coefficient of determination

RMSE - root mean squared error

`$$RMSE = \sqrt{\frac{1}{n}\sum_{i = 1}^{n}(y_{i} - \hat{y}_{i})^2}$$`

MAE - mean absolute error

`$$MAE = \frac{1}{n}\sum_{i = 1}^{n}|y_{i} - \hat{y}_{i}|$$`

---
class: inverse, middle, center
# The bias-variance tradeoff
---
### The bias-variance tradeoff

What is *bias*?

*bias* is the amount of error introduced by trying to estimate a complex relationship using a simpler model



.pull-left[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;
]
---
### The bias-variance tradeoff

What is *variance*?

*variance* is the amount that our model, `\(\large \hat{f}(X)\)` would change if we used a different training set.

.pull-left[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;
]
---
### The bias-variance tradeoff

What is *variance*?

*variance* is the amount that our model, `\(\large \hat{f}(X)\)` would change if we used a different training set.

.pull-left[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;
]
---
### The bias-variance tradeoff

Let's illustrate the tradeoff with this simple example:



&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;
---
### The bias-variance tradeoff

.pull-left[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;
]
---
### The bias-variance tradeoff



High bias, low variance

Testing set RMSE = 3.9638089

.pull-left[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;
]
---
### The bias-variance tradeoff



low bias, low variance

Testing set RMSE = 2.9951022

.pull-left[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;
]
---
### The bias-variance tradeoff



low bias, high variance

Testing set RMSE = 3.6155451

.pull-left[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="intro_to_data_science_session_7_3_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;" /&gt;
]
---
### The bias-variance tradeoff

We want a model with:

low bias and low variance

In practice, why would this be hard to find?

---
class: inverse, center, middle

# End of Session 7.3
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {window.dispatchEvent(new Event('resize'));});
(function() {var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler"); if (!r) return; s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }"; d.head.appendChild(s);})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
